This pipeline consists of 5 scripts:

- 1_Fast-SG.sh is a bash script that does several things but generally runs the Fast-SG program itself. It indexes the genome file to be scaffolded eventually, optionally filters this genome by contig size (recommended is 2kb, but I cant find a reason for that recommendation). It renames the long-reads to be used for scaffolding, and then runs Fast-SG, creating several sam files of the insert sizes specified in "ultra-long-conf.txt"

- 2_get-insert-size.sh is an awk one liner that, for every sam file fed into it (one at a time) produces a .insert.txt file with a list of the observed insert sizes. The insert size is also calculated in the log file of Fast-SG above, but it is a little tricky ot parse out for me. The .insert.txt file is used by a subsequent R script to plot the distributions of insert sizes.

- 3_boxplot.R takes the ultra-long-conf.txt file to determine the desired insert sizes, a manually generated "files-sort.txt" file with the sam file names, and the .insert.txt files from step 2 to make a boxplot of actual insert sizes. This boxplot can be useful for diagnosing which synthetic libraries are actually correct.

- 4_splitSAM.sh takes each sam file and divides it into forward and reverse reads

- 5_ScaffMatch.sh technically this doesn't need to be ScaffMatch; it could be any scaffolding program. This needs the name of each (forward and reverse) sam file, their insert size and stdev, and what sort of "library" they are (fr|rf). It is run in python 2 and so I made a conda environment with python2 and the program already loaded. This step takes about 2 days to run.